import streamlit as st
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

# Configuraci√≥n de la app
st.set_page_config(page_title="Generador Filos√≥fico en Espa√±ol", page_icon="üìö")
st.title("üìö Generador de Texto Filos√≥fico (Espa√±ol, local)")

# Carga del modelo en espa√±ol (desde Hugging Face)
@st.cache_resource
def cargar_modelo():
    modelo = GPT2LMHeadModel.from_pretrained("datificate/gpt2-small-spanish")
    tokenizer = GPT2Tokenizer.from_pretrained("datificate/gpt2-small-spanish")
    return modelo, tokenizer

modelo, tokenizer = cargar_modelo()

# Entrada del usuario
tema = st.text_input("üìù Tema filos√≥fico", placeholder="Ej: El tiempo y la existencia")
longitud = st.slider("üìè Longitud del texto", 50, 300, 150)

# Selector de estilo
estilo = st.selectbox("üß† Estilo filos√≥fico", ["General", "Socr√°tico", "Existencialista", "Estoico", "Idealismo alem√°n"])

# Construcci√≥n del prompt inicial
def construir_prompt(tema, estilo):
    estilos = {
        "General": f"Reflexionando sobre {tema}, uno puede considerar que",
        "Socr√°tico": f"‚Äì S√≥crates: ¬øQu√© es {tema}?\n‚Äì Interlocutor:",
        "Existencialista": f"En un mundo sin sentido, {tema} se convierte en reflejo de la angustia del ser.",
        "Estoico": f"Un sabio estoico contempla {tema} con serenidad y raz√≥n.",
        "Idealismo alem√°n": f"Seg√∫n la raz√≥n pura, {tema} es una manifestaci√≥n del esp√≠ritu absoluto.",
    }
    return estilos.get(estilo, f"Reflexi√≥n sobre {tema}:")

# Bot√≥n para generar
if st.button("Generar reflexi√≥n"):
    if not tema:
        st.warning("Por favor, escribe un tema.")
    else:
        prompt = construir_prompt(tema, estilo)

        input_ids = tokenizer.encode(prompt, return_tensors="pt")

        # Generar texto
        with torch.no_grad():
            salida = modelo.generate(
                input_ids,
                max_length=len(input_ids[0]) + longitud,
                num_return_sequences=1,
                no_repeat_ngram_size=2,
                temperature=0.95,
                top_k=50,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )

        texto_generado = tokenizer.decode(salida[0], skip_special_tokens=True)
        st.markdown("### üßæ Reflexi√≥n generada")
        st.write(texto_generado)
